{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e9d5c8",
   "metadata": {},
   "source": [
    "# Text Analysis Group Assignment\n",
    "    \n",
    "    AJITH SANIKOMMU       PGID: 12120006\n",
    "    Anjana Rajan\t      PGID: 12120080\n",
    "    Bhargavi Peddapati\t  PGID: 12120067\n",
    "    Rohini Singh\t      PGID: 12120059\n",
    "    Shantanu Srivastava\t  PGID: 12120061"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f6d84",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b18f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load/import libraries  3.496\n"
     ]
    }
   ],
   "source": [
    "#!pip install vaderSentiment\n",
    "#!pip install wordcloud\n",
    "#!pip install -U gensim\n",
    "#!pip install xgboost\n",
    "#!pip install plotly\n",
    "#!pip install textblob\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#!pip install seaborn\n",
    "import time\n",
    "t0 = time.time()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from pprint import pprint\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import ensemble\n",
    "import xgboost, string \n",
    "import csv,nltk\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from gensim.parsing.preprocessing import strip_punctuation, strip_tags, strip_numeric\n",
    "from nltk.stem.wordnet import WordNetLemmatizer   \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time taken to load/import libraries \", round(t1-t0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925f0d7",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da6cb60",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'modified_uber_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14896/2017515775.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muber_ride_reviews_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"modified_uber_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'unicode_escape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0muber_ride_reviews_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'modified_uber_data.csv'"
     ]
    }
   ],
   "source": [
    "uber_ride_reviews_df = pd.read_csv(\"modified_uber_data.csv\", encoding= 'unicode_escape')\n",
    "uber_ride_reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd4d04",
   "metadata": {},
   "source": [
    "# Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa48b6",
   "metadata": {},
   "source": [
    "<b> To improve the uber services, Uber wants to study customer reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21108918",
   "metadata": {},
   "source": [
    "# 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41443c",
   "metadata": {},
   "source": [
    "## Examine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefba158",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_ride_reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0da51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_ride_reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_ride_reviews_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c912147",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_ride_reviews_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_ride_reviews_df['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb072d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_ride_reviews_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b465fea6",
   "metadata": {},
   "source": [
    "## Identifing the columns of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f03de9",
   "metadata": {},
   "source": [
    "Here 'Title', 'Review', 'Rating', 'Date' columns are interested for analysing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368a170",
   "metadata": {},
   "source": [
    "## Cleaning of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef5635",
   "metadata": {},
   "source": [
    "### Checking nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b60fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_ride_reviews_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2557c4",
   "metadata": {},
   "source": [
    "### Drop Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(dataCol):\n",
    "    r = re.compile(r'[^a-zA-Z !@#$%&*_+-=|\\:\";<>,./()[\\]{}\\']')\n",
    "    return r.sub('', dataCol)\n",
    "\n",
    "uber_ride_reviews_df['Review'] = uber_ride_reviews_df['Review'].apply(remove_special_characters)\n",
    "uber_ride_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa88dec",
   "metadata": {},
   "source": [
    "### Cleanup html junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_junk(dataCol):\n",
    "    text = BeautifulSoup(dataCol, 'html.parser').getText()\n",
    "    return text\n",
    "\n",
    "uber_ride_reviews_df['Review'] = uber_ride_reviews_df['Review'].apply(remove_html_junk)\n",
    "uber_ride_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065f1e7",
   "metadata": {},
   "source": [
    "### Trim Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_string(dataCol):\n",
    "    return dataCol.strip()\n",
    "\n",
    "uber_ride_reviews_df['Review'] = uber_ride_reviews_df['Review'].apply(trim_string)\n",
    "uber_ride_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e069d862",
   "metadata": {},
   "source": [
    "### Remove certain patterns observed in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08919360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def obs_pattern(dataCol):\n",
    "    r = re.compile(r'^[\\s!\\s]+$')\n",
    "    return r.sub('', dataCol)\n",
    "\n",
    "uber_ride_reviews_df['Review'] = uber_ride_reviews_df['Review'].apply(obs_pattern)\n",
    "uber_ride_reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13401aaf",
   "metadata": {},
   "source": [
    "# 2. Basic text and sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce0384",
   "metadata": {},
   "source": [
    "## Basic Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df61b5",
   "metadata": {},
   "source": [
    "<b>Load nltk's English stopswords</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e47ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a5210",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8effa906",
   "metadata": {},
   "source": [
    "<b>Below are the basic tokenization steps\n",
    "- <b>Tokenize sentence into words\n",
    "- <b>Convert words to lowercase\n",
    "- <b>Consider only Alphanumeric words and exclude other's like numeric etc.\n",
    "- <b>Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if(not token in stopwords):            \n",
    "            if re.search('[a-zA-Z]',token):\n",
    "                filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_tokenized =[]\n",
    "for i in uber_ride_reviews_df['Review']:    \n",
    "    allword_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allword_tokenized) \n",
    "print(\"Get 10 words from list \",totalvocab_tokenized[:10])\n",
    "print(\"Total number of tokenized list \", len(totalvocab_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e642b5",
   "metadata": {},
   "source": [
    "### Tokenization With Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18da038",
   "metadata": {},
   "source": [
    "<b>Load nltk's stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a283c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cba49",
   "metadata": {},
   "source": [
    "<b>Below are the basic tokenization steps\n",
    "- <b>Call tokenize_only method to get list of tokens\n",
    "- <b>Get stem of each word\n",
    "- <b>Get final list of Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44237f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):    \n",
    "    filtered_tokens = tokenize_only(text)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f48e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "for i in uber_ride_reviews_df['Review']:\n",
    "    allwords_stemmed = tokenize_and_stem(i)\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "totalvocab_stemmed\n",
    "print(\"After stemming : \")\n",
    "print(\"Get 10 words from list \",totalvocab_stemmed[:10])\n",
    "print(\"Total number of tokenized list \", len(totalvocab_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030e5bb",
   "metadata": {},
   "source": [
    "### Parts Of Speech "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a990867c",
   "metadata": {},
   "source": [
    "- <b>On Review column apply tokenization, Removal of stopword, POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001cc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(tokenize_only(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        newlist.append(tuple([word, pos_dict.get(tag[0])]))           \n",
    "    return newlist\n",
    "\n",
    "uber_ride_reviews_df['POS tagged'] = uber_ride_reviews_df['Review'].apply(token_stop_pos)\n",
    "uber_ride_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af3d63",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d512b",
   "metadata": {},
   "source": [
    "- <b>Apply Lemmatization on the POS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c96e40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "uber_ride_reviews_df['Lemma'] = uber_ride_reviews_df['POS tagged'].apply(lemmatize)\n",
    "uber_ride_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec08e66",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de475cfc",
   "metadata": {},
   "source": [
    "- <b>Defined a function to calculate Sentiment polarity score using vader\n",
    "- <b>Create new column 'Vader Sentiment' apply on Lemma Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c43cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def vadersentimentanalysis(review):\n",
    "    vs = analyzer.polarity_scores(review)\n",
    "    return vs['compound']\n",
    "uber_ride_reviews_df['Vader Sentiment'] = uber_ride_reviews_df['Lemma'].apply(vadersentimentanalysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e2d9d3",
   "metadata": {},
   "source": [
    "- <b>Defined a function for Labeling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_analysis(compound):\n",
    "    if compound > 0:\n",
    "        return 'Positive'\n",
    "    elif compound == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "uber_ride_reviews_df['Vader Analysis'] = uber_ride_reviews_df['Vader Sentiment'].apply(vader_analysis)\n",
    "uber_ride_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth',500)\n",
    "rslt_df = uber_ride_reviews_df.loc[uber_ride_reviews_df['Vader Analysis'] == 'Negative'] \n",
    "rslt_df[['Review','Vader Analysis']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_df = uber_ride_reviews_df.loc[uber_ride_reviews_df['Vader Analysis'] == 'Neutral'] \n",
    "rslt_df[['Review','Vader Analysis']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_df = uber_ride_reviews_df.loc[uber_ride_reviews_df['Vader Analysis'] == 'Positive'] \n",
    "rslt_df[['Review','Vader Analysis']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a4d5ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentiment_analysis = uber_ride_reviews_df.groupby(['Vader Analysis']).size().reset_index(name='counts')\n",
    "sentiment_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9a31c",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis Conclusion\n",
    "\n",
    "After adjusting compound value to greater than 0 for negative values,we are more number of negative reviews.\n",
    "\n",
    "The downside of this analysis is that we are getting higher number of positives as well which in reality is not true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2bc70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8, 5)) \n",
    "# creating the bar plot\n",
    "plt.bar(sentiment_analysis['Vader Analysis'], sentiment_analysis['counts'], color=['red','gray','green'], width = 0.4) \n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Sentiments Counts\")\n",
    "plt.title(\"Sentiment Analysis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e573f",
   "metadata": {},
   "source": [
    "### Defining Unit Func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea67eb4",
   "metadata": {},
   "source": [
    "- <b>Defined unit method to process one document\n",
    "- <b>Test it for first Review record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfdfa9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vader_unit_func(doc0):\n",
    "    sents_list0 = tokenize_only(doc0)\n",
    "    vs_doc0 = []\n",
    "    sent_ind = []\n",
    "    for i in range(len(sents_list0)):\n",
    "        vs_sent0 = analyzer.polarity_scores(sents_list0[i])\n",
    "        vs_doc0.append(vs_sent0)\n",
    "        sent_ind.append(i)\n",
    "        \n",
    "    # obtain output as DF    \n",
    "    doc0_df = pd.DataFrame(vs_doc0)\n",
    "    doc0_df.insert(0, 'sent_index', sent_ind)  # insert sent index\n",
    "    doc0_df.insert(doc0_df.shape[1], 'sentence', sents_list0)\n",
    "    return(doc0_df)\n",
    "\n",
    "%time doc0_df = vader_unit_func(uber_ride_reviews_df['Review'].iloc[0])\n",
    "doc0_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c5785",
   "metadata": {},
   "source": [
    "### Defining Wrapper-Func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155da007",
   "metadata": {},
   "source": [
    "- <b>Deinfe Wrapper function to get unit function to each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2650f268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define wrapper func\n",
    "def vader_wrap_func(corpus0):\n",
    "    \n",
    "    # use ifinstance() to check & convert input to DF\n",
    "    if isinstance(corpus0, list):\n",
    "        corpus0 = pd.DataFrame({'text':corpus0})\n",
    "    \n",
    "    # define empty DF to concat unit func output to\n",
    "    vs_df = pd.DataFrame(columns=['doc_index', 'sent_index', 'neg', 'neu', 'pos', 'compound', 'sentence'])    \n",
    "    \n",
    "    # apply unit-func to each doc & loop over all docs\n",
    "    for i1 in range(len(corpus0)):\n",
    "        doc0 = str(corpus0.Review.iloc[i1])\n",
    "        vs_doc_df = vader_unit_func(doc0)  # applying unit-func\n",
    "        vs_doc_df.insert(0, 'doc_index', i1)  # inserting doc index\n",
    "        vs_df = pd.concat([vs_df, vs_doc_df], axis=0)\n",
    "        \n",
    "    return(vs_df)\n",
    "\n",
    "# test-drive wrapper func\n",
    "%time uber_ride_reviews_vs_df = vader_wrap_func(uber_ride_reviews_df)    \n",
    "uber_ride_reviews_vs_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ec5d5",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ba41f",
   "metadata": {},
   "source": [
    "#### Valence Anlaysis\n",
    "\n",
    "We are not getting a distinguishable difference between positive and negative Reviews using Vader Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a3d02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define X and Y variable data\n",
    "#x = np.array([1, 2, 3, 4])\n",
    "y = uber_ride_reviews_vs_df.compound\n",
    "x = pd.Series([x for x in range(len(uber_ride_reviews_vs_df))])\n",
    "fig = plt.figure(figsize = (19, 8))   \n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Sentence num\")  # add X-axis label\n",
    "plt.ylabel(\"Compound valence score\")  # add Y-axis label\n",
    "plt.title(\"Valence of Uber Review by sentence\")  # add title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6142b5e",
   "metadata": {},
   "source": [
    "## Ngrams - (Bigrams, Trigrams.. etc), COGs, wordclouds, phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80e867",
   "metadata": {},
   "source": [
    "### Ngrams - (Bigrams, Trigrams.. etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7f8b7",
   "metadata": {},
   "source": [
    "- <b>NGrams_Generator method will generate n-grams where is dynamic\n",
    "- <b>We can generate Ngrams by sending n value as 1,2,3... etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NGrams_Generator(testlist,ngram=1):    \n",
    "    columns = []\n",
    "    for w in range(1,ngram+1):       \n",
    "        columns.append('Word'+str(w))\n",
    "        \n",
    "    ngramsdf = pd.DataFrame(columns = columns)  \n",
    "    for t in testlist:           \n",
    "        words=tokenize_only(t)\n",
    "        ngrams=zip(*[words[i:] for i in range(0,ngram)])\n",
    "        df = pd.DataFrame(ngrams,columns =columns)       \n",
    "        ngramsdf= ngramsdf.append(df,ignore_index = True)       \n",
    "    return ngramsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d099e",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffac67e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigrams_df = NGrams_Generator(uber_ride_reviews_df['Review'],2)\n",
    "print(\"Bgram\")\n",
    "bigrams_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83282f7",
   "metadata": {},
   "source": [
    "#### Bigram Analysis \n",
    "Customer Service, Uber Use, Uber Eats, Uber Pass are the most occuring features we have identifed using Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6810de30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bi_count_df = bigrams_df.groupby(['Word1','Word2']).size().reset_index(name='counts').sort_values(['counts'], ascending=False)\n",
    "bi_count_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f579f0d",
   "metadata": {},
   "source": [
    "#### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da19d18b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trigrams_df = NGrams_Generator(uber_ride_reviews_df['Review'],3)\n",
    "print(\"Trigram\")\n",
    "trigrams_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a216f7",
   "metadata": {},
   "source": [
    "#### Trigram Analysis \n",
    "mostly money, timing and charging related issues can be identified from Trigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3cc94c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trigrams_df = trigrams_df.groupby(['Word1','Word2','Word3']).size().reset_index(name='counts').sort_values(\n",
    "    ['counts'], ascending=False)\n",
    "trigrams_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b11c53",
   "metadata": {},
   "source": [
    "# 3. Feature construction and extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8aadd",
   "metadata": {},
   "source": [
    "### Construct DTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664245c",
   "metadata": {},
   "source": [
    "- <b>Build a panda DF, index as stemmed vocabulary and column as tokenized words\n",
    "- <b>Build DTM under TFIDF scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb768b7d",
   "metadata": {},
   "source": [
    "- <b>1. Build/Construct vectorizer by providing Lemmatized Text, n value to calculate Ngram \n",
    "- <b>2. Transform vectorizer to get BOW, feature names\n",
    "- <b>3. Construct dataframe of bag of words\n",
    "- <b>4. Build TFIDF matrix from bag of words\n",
    "- <b>5. Get Words of highest weights\n",
    "- <b>6. Build dataframe with word,count,weight\n",
    "- <b>7. Get Cosine similarity\n",
    "- <b>8. Get the Distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a481463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVector(text,ngram):\n",
    "    cvector = CountVectorizer(max_df=1.0,  min_df=0.0, vocabulary=None, ngram_range=(1,ngram))\n",
    "    cvectorfit = cvector.fit(text)\n",
    "    \n",
    "    bag_of_words = cvectorfit.transform(text)\n",
    "    feature_names = cvectorfit.get_feature_names()\n",
    "    bag_of_words_df = pd.DataFrame(bag_of_words.todense(), columns=feature_names)\n",
    "    \n",
    "    tfidftransformer = TfidfTransformer()\n",
    "    tfidf = tfidftransformer.fit_transform(bag_of_words)\n",
    "    \n",
    "    word_cnts = np.asarray(bag_of_words.sum(axis=0)).ravel().tolist()\n",
    "    df_cnts = pd.DataFrame({'word': feature_names, 'count': word_cnts})\n",
    "    df_cnts = df_cnts.sort_values('count', ascending=False)\n",
    "    weights = np.asarray(tfidf.mean(axis=0)).ravel().tolist()\n",
    "    df_weights = pd.DataFrame({'word': feature_names, 'weight': weights})\n",
    "    df_weights = df_weights.sort_values('weight', ascending=False)\n",
    "\n",
    "    df_weights = df_weights.merge(df_cnts, on='word', how='left')\n",
    "    df_weights = df_weights[['word', 'count', 'weight']]\n",
    "\n",
    "    cos_sim = cosine_similarity(tfidf, tfidf)\n",
    "    samp_dist = 1 - cos_sim\n",
    "    \n",
    "    return cvectorfit, feature_names, bag_of_words_df, tfidf, df_weights, cos_sim, samp_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec, feature_names, bag_of_words_df, tfidf, df_weights, cos_sim, samp_dist = buildVector(uber_ride_reviews_df['Lemma'],2)\n",
    "df_tfidf = pd.DataFrame(tfidf.todense(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40253fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Features:\")\n",
    "feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weights:\")\n",
    "df_weights.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412c559",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences = uber_ride_reviews_df['Lemma'].values.tolist()\n",
    "print(\"cos_sim[%d,%d] (a square matrix of length and width ) \" % (len(sentences), len(sentences)))\n",
    "df = pd.DataFrame(np.array(cos_sim))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_bag_of_words[%d,%d]:\" % (len(sentences), len(feature_names)))\n",
    "bag_of_words_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4a7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_word_freq = pd.Series(df_weights['count'])\n",
    "s_word_freq.index = df_weights['word']\n",
    "di_word_freq = s_word_freq.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1a002",
   "metadata": {},
   "source": [
    "### Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819913bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(text):\n",
    "    wordcloud = WordCloud(\n",
    "        width = 3000,\n",
    "        height = 2000,\n",
    "        background_color = 'white').generate_from_frequencies(text)\n",
    "    fig = plt.figure(\n",
    "        figsize = (15, 8),\n",
    "        facecolor = 'k',\n",
    "        edgecolor = 'k')\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8c7db",
   "metadata": {},
   "source": [
    "#### Word Cloud Analysis\n",
    "\n",
    "The most prominent features we can identify are:\n",
    "\n",
    "1. Uber\n",
    "2. Driver\n",
    "3. Ride\n",
    "4. App\n",
    "5. Charge\n",
    "\n",
    "Since, 90% of the ratings are negative, we can say that the features mentioned above are very likely to be Issue related or Negative\n",
    "\n",
    "###### Recommendations - Driver, Ride, App & Charging related issues must be addressed as these words frequency is the highest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud(di_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec788665",
   "metadata": {},
   "source": [
    "### PCA - Dimensionality Reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b1ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction using PCA, reduce the tfidf matrix to just 2 features\n",
    "X = tfidf.todense()\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "print(\"X_pca now has just 2 columns:\")\n",
    "print(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bed0b5",
   "metadata": {},
   "source": [
    "### K-means clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = np.unique(uber_ride_reviews_df['Title']).shape[0]\n",
    "print(\"n_clusters:\", n_clusters)\n",
    "\n",
    "km_model = KMeans(n_clusters=n_clusters, max_iter=10, n_init=2, random_state=0)\n",
    "\n",
    "# K-means (from number of features in input matrix to n_clusters)\n",
    "km_model.fit(X_pca)\n",
    "df_centers = pd.DataFrame(km_model.cluster_centers_, columns=['x', 'y'])\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.suptitle('PCA features colored by class; grey circles show the k-means centers')\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=km_model.labels_, s=50, cmap='jet')\n",
    "plt.scatter(df_centers['x'], df_centers['y'], c='grey', s=500, alpha=0.2);\n",
    "\n",
    "dy = 0.04\n",
    "for i, txt in enumerate(km_model.labels_):\n",
    "    plt.annotate(txt, (X_pca[i, 0], X_pca[i, 1] + dy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6af06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"km_model.labels_:\", km_model.labels_)\n",
    "print(\"This corresponds to the sentence labels shown below as follows:\")\n",
    "print(uber_ride_reviews_df['Title'].tolist())\n",
    "print(\"---\")\n",
    "print(\"df_centers:\")\n",
    "print(df_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b44bbf",
   "metadata": {},
   "source": [
    "### Implementation of LDA using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=6,max_iter=20,random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtopic = lda_model.fit_transform(tfidf)\n",
    "xtopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d01a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words =lda_model.components_\n",
    "beta_df = pd.DataFrame(topic_words)\n",
    "topicNames=['topic' + format(x+1, '02d') for x in range(len(beta_df))]\n",
    "topicNames_series = pd.Series(topicNames)\n",
    "final = beta_df.iloc[:8, :8].T\n",
    "final.rename(columns=topicNames_series, inplace=True)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a343aa1b",
   "metadata": {},
   "source": [
    "### Compute Fit metrics for topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words=5\n",
    "for i,topic_dist in enumerate(topic_words):\n",
    "    sorted_topic_dist = np.argsort(topic_dist)\n",
    "    topic_words = np.array(feature_names)[sorted_topic_dist]\n",
    "    topic_words = topic_words[:n_top_words:-1]\n",
    "    print(\"Topic\",str(i+1),topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be322158",
   "metadata": {},
   "source": [
    "#### LDA Analysis\n",
    "\n",
    "The topics we have identified from LDA are following:\n",
    "\n",
    "Topic 1 ['driver' 'charge' 'uber' ... 'app informs' 'stop service' 'go work']\n",
    "Topic 2 ['app' 'uber' 'service' ... 'plane' 'informs' 'app informs']\n",
    "Topic 3 ['ride' 'uber' 'get' ... 'informs' 'app informs' 'stop service']\n",
    "Topic 4 ['uber' 'ride' 'get' ... 'reserve time' 'perfect' 'training']\n",
    "Topic 5 ['uber' 'use' 'driver' ... 'informs' 'stop service' 'gig']\n",
    "Topic 6 ['uber' 'driver' 'get' ... 'app informs' 'informs' 'stop service']\n",
    "\n",
    "The drawback of LDA is that we do not get the weights of each keyword associated with each Topic. Therefore, to differentiate a topic is difficult in LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a3511",
   "metadata": {},
   "source": [
    "### Annotating the topics the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb720a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_topic = lda_model.transform(tfidf)\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    topic_doc =doc_topic[n].argmax()\n",
    "    print(\"Document\",n+1,\"Topic \", topic_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ce5e8",
   "metadata": {},
   "source": [
    "### Implementation of LDA using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da261f",
   "metadata": {},
   "source": [
    "We are using Gensim Topic modelling over LDA for running classification because LDA has limitations such as\n",
    "1. Fixed K - we need to know the number of topics beforehand\n",
    "2. Uncorrelated Topics - correlations are not captured using LDA\n",
    "\n",
    "In Gensim model, we are trying to explore as many features as the model can identify using coherence values and perplexity fitb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6edc0",
   "metadata": {},
   "source": [
    "#### Step 1: Text cleaning and Tokenisation\n",
    "\n",
    "Clean the text and then remove any additional stop words\n",
    "NLTK extended Stop words list\n",
    "Below we are removing common words that do not provide latent feature specific words. \n",
    "Words like uber, driver, couldnt, taking, dude, something etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070255e3",
   "metadata": {},
   "source": [
    "### Please note that we are running Gensim Model and Text Classification for Reviews with Ratings 1 or 2\n",
    "\n",
    "We want to identify the features that are contributing to Bad Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "uber_ride_reviews_df['Review']=uber_ride_reviews_df['Review'].str.replace('driver', '')\n",
    "uber_ride_reviews_df = uber_ride_reviews_df[(uber_ride_reviews_df.Rating==1)|(uber_ride_reviews_df.Rating==2)]\n",
    "# NLTK Stop words extended\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['uber','away', 'every','certain','finally','got','get', 'im','even','cant', 'would',\n",
    "                  'said', 'could', 'wanted', 'needed', 'end', 'taking', 'dude','gotten', 'since', 'le', 'thus',\n",
    "                  'tx', 'whatever', 'therefore','ever', 'pa', 'wasnt','still', 'gave','yall','something', 'theyre',\n",
    "                  'dont', 'need',  'wont', 'doesnt',  'nothing', 'ive','saying','u','please'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "def textClean(text0):\n",
    "    text1 = [strip_punctuation(doc) for doc in text0]\n",
    "    text1 = [strip_tags(doc) for doc in text1]\n",
    "    text1 = [strip_numeric(doc) for doc in text1]\n",
    "    text1 = [[\" \".join([i for i in doc.lower().split() if i not in stop_words])] for doc in text1]\n",
    "    text2 = [[word for word in ' '.join(doc).split()] for doc in text1]\n",
    "    normalized = [[\" \".join([lemma.lemmatize(word) for word in ' '.join(doc).split()])] for doc in text1]\n",
    "    return normalized\n",
    "\n",
    "corpus1 = textClean(uber_ride_reviews_df['Review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6025e6",
   "metadata": {},
   "source": [
    "#### Step 2: Creating corpus for Topic Modelling\n",
    "\n",
    "Gensim supports building of dictionary object which we are applying on n-gram we have built.\n",
    "\n",
    "After, creating the dictionary, we are creating the TF or term frequency count based on DTM \n",
    "\n",
    "We have kept the min_count and threshold of these values on a bit higher side to get better bigrams from the Reviews as the reviews are quite descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DTM building etc via gensim\n",
    "corpus2 = [[word for word in ' '.join(doc).split()] for doc in corpus1]  # word_tokenize first\n",
    "\n",
    "# Building the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(corpus2, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[corpus2], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# print(trigram_mod[bigram_mod[corpus2[0]]])  # See trigram example\n",
    "\n",
    "id2word = corpora.Dictionary(corpus2)  # Create Dictionary\n",
    "corpus = [id2word.doc2bow(text) for text in corpus2]  # Building gensim corpus. TF DTM creation.\n",
    "print(corpus[:1])  # View one doc in abstract form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "a0 = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "[x for (x, y) in a0[0] if y >4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52ce9f",
   "metadata": {},
   "source": [
    "#### Step 3: Building the Topic Model\n",
    "\n",
    "we have given the number of topics as 4 to extract latent topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b440b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model for (say) K=4 topics\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f308f6",
   "metadata": {},
   "source": [
    "#### Step 4: Obtain Factor Matrices\n",
    "\n",
    "We obtain two factor Matrices from Gensim data\n",
    "1. beta - maps tokens to topics\n",
    "2. gamma - maps docs to topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f57879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## def func for beta_df\n",
    "import pandas as pd\n",
    "\n",
    "## obtain the factor matrices - beta\n",
    "def build_beta_df(lda_model=lda_model, id2word=id2word):\n",
    "    beta = lda_model.get_topics()  # shape (num_topics, vocabulary_size).\n",
    "    beta_df = pd.DataFrame(data=beta)\n",
    "\n",
    "    # convert colnames in beta_df 2 tokens\n",
    "    token2col = list(id2word.token2id)\n",
    "    beta_df.columns = token2col\n",
    "    # beta_df.loc[0,:].sum()  # checking if rows sum to 1\n",
    "\n",
    "    # convert rownames too, eh? Using format(), .shape[] and range()\n",
    "    rowNames=['topic' + format(x+1, '02d') for x in range(beta_df.shape[0])]\n",
    "    rowNames_series = pd.Series(rowNames)\n",
    "    beta_df.rename(index=rowNames_series, inplace=True)\n",
    "    return(beta_df)\n",
    "\n",
    "# invoke func\n",
    "beta_df = build_beta_df(lda_model=lda_model, id2word=id2word)\n",
    "beta_df.iloc[:8, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b37dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus0 = uber_ride_reviews_df['Review'].to_list()\n",
    "# func to get gamma matrix by looping using list.comp\n",
    "def build_gamma_df(lda_model, corpus0):\n",
    "    gamma_doc = []  # empty list 2 populate with gamma colms\n",
    "    num_topics = lda_model.get_topics().shape[0]\n",
    "    \n",
    "    for doc in range(len(corpus0)):\n",
    "        doc1 = corpus0[doc].split()\n",
    "        bow_doc = id2word.doc2bow(doc1)\n",
    "        gamma_doc0 = [0]*num_topics  # define list of zeroes num_topics long\n",
    "        gamma_doc1 = lda_model.get_document_topics(bow_doc)\n",
    "        gamma_doc2_x = [x for (x,y) in gamma_doc1]#; gamma_doc2_x\n",
    "        gamma_doc2_y = [y for (x,y) in gamma_doc1]#; gamma_doc2_y\n",
    "        for i in range(len(gamma_doc1)):\n",
    "            x = gamma_doc2_x[i]\n",
    "            y = gamma_doc2_y[i]\n",
    "            gamma_doc0[x] = y  # wasn't geting this in list comprehension somehow \n",
    "        gamma_doc.append(gamma_doc0)\n",
    "        \n",
    "    gamma_df = pd.DataFrame(data=gamma_doc)  # shape=num_docs x num_topics\n",
    "    topicNames=['topic' + format(x+1, '02d') for x in range(num_topics)]\n",
    "    topicNames_series = pd.Series(topicNames)\n",
    "    gamma_df.rename(columns=topicNames_series, inplace=True)\n",
    "    return(gamma_df)\n",
    "\n",
    "# now apply func\n",
    "gamma_df = build_gamma_df(lda_model=lda_model, corpus0=corpus0)\n",
    "gamma_df.iloc[:8, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4e32c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma_df.iloc[2,:].sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37638c5e",
   "metadata": {},
   "source": [
    "#### Step 6: Compute fit metrics for Topic Models\n",
    "\n",
    "We iterate the model with varying num_topics to obtain optimal number of topics. \n",
    "\n",
    "Perplexity score tells us the model fitness. \n",
    "\n",
    "By running this model on different num_topics, the final perplexity score we are achieving is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a278f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # model fit metric. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e816ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute coherence score (akin to LMD?)\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    num_topics1 = [i for i in range(start, limit, step)]\n",
    "    for num_topics in num_topics1:\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics, random_state=100,\n",
    "                                           update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values  # note, list of 2 objs returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dcd004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "start1=2\n",
    "limit1=19\n",
    "step1=1\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, \n",
    "                                                        texts=corpus2, start=start1, limit=limit1, step=step1)\n",
    "\n",
    "print(coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e8d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain optimal topic number\n",
    "coher = list(enumerate(coherence_values))  # create an index for each list elem\n",
    "index_max = [x for (x,y) in coher if y==max(coherence_values)]  # obtain index num corres to max coherence value\n",
    "Optimal_numTopics = int(str(index_max[0]))+2  # convert that list elem into integer (int()) via string (str())\n",
    "print(Optimal_numTopics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be77e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the change in coherence score with num_topics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "start1=2\n",
    "limit1=19\n",
    "step1=1\n",
    "# Show graph\n",
    "x = range(start1, limit1, step1)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.axvline(x=Optimal_numTopics, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b82b54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd8268",
   "metadata": {},
   "source": [
    "#### Interpreting optimal topics\n",
    "\n",
    "We have built the LDA model with 10 different topics in which multiple keywords contribute to a certain weightage(importance) for that topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4a5a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# seems optimal num_topics is 10\n",
    "optimal_model = model_list[Optimal_numTopics]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=17))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258ee40",
   "metadata": {},
   "source": [
    "#### Step7: Exploring topics obtained from optimal model\n",
    "\n",
    "We are identifying the dominant topic and then mapping it back with the main document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f43ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get main topic in each document\n",
    "gamma_df = build_gamma_df(lda_model=optimal_model, corpus0=corpus0)\n",
    "#gamma_df.iloc[:8,:8]\n",
    "\n",
    "row0 = gamma_df.values.tolist()\n",
    "row=[]\n",
    "for i in range(len(row0)):\n",
    "    row1 = list(enumerate(row0[i]))\n",
    "    row1_y = [y for (x,y) in row1]\n",
    "    max_propn = sorted(row1_y, reverse=True)[0]\n",
    "    row2 = [(i, x, y) for (x, y) in row1 if y==max_propn]\n",
    "    row.append(row2)\n",
    "\n",
    "row[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad21f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "for row1 in row:\n",
    "    for (doc_num, topic_num, prop_topic) in row1:\n",
    "        wp = optimal_model.show_topic(topic_num)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        sent_topics_df = sent_topics_df.append(pd.Series([int(doc_num), int(topic_num), \n",
    "                                                          round(prop_topic,4), \n",
    "                                                          topic_keywords]), \n",
    "                                                       ignore_index=True)\n",
    "    \n",
    "sent_topics_df.columns = ['Doc_num', 'Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']    \n",
    "sent_topics_df.iloc[:8, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411cc46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add original text to the end of the output\n",
    "contents = pd.Series(corpus0)\n",
    "sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "#return(sent_topics_df)\n",
    "sent_topics_df.columns = ['Doc_num', 'Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'contents']\n",
    "sent_topics_df.iloc[:8,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae4320",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = sent_topics_df['Dominant_Topic'].value_counts()\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86a037",
   "metadata": {},
   "source": [
    "#### Conclusion of Gensim Topic Model\n",
    "\n",
    "We have identified 10 dominant topics with following interpretation\n",
    "\n",
    "#Topic Descriptors sorted by size or count of reviews\n",
    " 3 --> Service (related issues)\n",
    " 2 --> Ride (related issues)\n",
    " 1 --> App (related issues)\n",
    " 11 --> Payment (related issues)\n",
    " 10 --> Advertising (related issues)\n",
    " 6 --> Family_Account (related issues)\n",
    " 4 --> Pricing (related issues)\n",
    " 5 --> Vehicle (related issues)\n",
    " 7 --> Experience (related issues)\n",
    " 9 --> Policy (related issues)\n",
    " 0 --> Availability (related issues)\n",
    " 8 --> Reliability (related issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d9e9c",
   "metadata": {},
   "source": [
    "# 4. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e15ea3",
   "metadata": {},
   "source": [
    "#### Step 1 : Utilising topics obtained from Gensim model for Text Classification \n",
    "\n",
    "We are utilising the topics we have obtained from Gensim for Text classification of Reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = sent_topics_df[['contents','Dominant_Topic']].dropna(how = 'any')\n",
    "print(data_subset.shape)\n",
    "data_subset['Dominant_Topic'] = data_subset['Dominant_Topic'].astype(int)\n",
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naming the Dominant Topics\n",
    "#Topic Descriptors sorted by size or count of reviews\n",
    "# 3 --> Service (related issues)\n",
    "# 2 --> Ride (related issues)\n",
    "# 1 --> App (related issues)\n",
    "# 11 --> Payment (related issues)\n",
    "# 10 --> Advertising (related issues)\n",
    "# 6 --> Family_Account (related issues)\n",
    "# 4 --> Pricing (related issues)\n",
    "# 5 --> Vehicle (related issues)\n",
    "# 7 --> Experience (related issues)\n",
    "# 9 --> Policy (related issues)\n",
    "# 0 --> Availability (related issues)\n",
    "# 8 --> Reliability (related issues)\n",
    "\n",
    "data_subset['Topic_desc'] = np.where(data_subset['Dominant_Topic']==0, 'Avalibility',\n",
    "                   np.where(data_subset['Dominant_Topic']==1, 'App',\n",
    "                   np.where(data_subset['Dominant_Topic']==2, 'Ride', \n",
    "                   np.where(data_subset['Dominant_Topic']==3, 'Service',\n",
    "                   np.where(data_subset['Dominant_Topic']==4, 'Pricing',\n",
    "                   np.where(data_subset['Dominant_Topic']==5, 'Vehicle',\n",
    "                   np.where(data_subset['Dominant_Topic']==6, 'Account',\n",
    "                   np.where(data_subset['Dominant_Topic']==7, 'Experience',\n",
    "                   np.where(data_subset['Dominant_Topic']==8, 'Reliability',\n",
    "                   np.where(data_subset['Dominant_Topic']==9, 'Policy',\n",
    "                   np.where(data_subset['Dominant_Topic']==10, 'Advertising','Payment')))))))))))\n",
    "data_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab9205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ensure records are unique\n",
    "categories = list(data_subset['Topic_desc'].drop_duplicates())\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038abed6",
   "metadata": {},
   "source": [
    "#### Step2: Feature Extraction\n",
    "\n",
    "We are using Bag of Words to compute tf and tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extraction\n",
    "# We would be filtering the words which occur in less than <2% of the documents and >98% of the documents \n",
    "# since these dont add much value to our analysis\n",
    "\n",
    "tf = CountVectorizer(min_df = 0.02 , max_df= 0.98, stop_words='english')\n",
    "tfidf = TfidfVectorizer(min_df = 0.02 , max_df= 0.98, stop_words='english')\n",
    "\n",
    "%time dtm = pd.DataFrame(tf.fit_transform(data_subset.iloc[:,0]).toarray(),columns=tf.get_feature_names()) # 12.3s\n",
    "%time dtm_idf = pd.DataFrame(tfidf.fit_transform(data_subset.iloc[:,0]).toarray(),columns=tfidf.get_feature_names()) # 12.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b102c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.head() # what does the DTM look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c9363",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b505b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_subset.iloc[:,-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4573ea",
   "metadata": {},
   "source": [
    "#### Step 3: Training the model\n",
    "\n",
    "Firstly, we need to split the data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(dtm,data_subset.iloc[:,-2])\n",
    "X_train_idf, X_test_idf, y_train_idf, y_test_idf = train_test_split(dtm_idf,data_subset.iloc[:,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7755950d",
   "metadata": {},
   "source": [
    "#### Naive Bayes Model Interpretation:\n",
    "\n",
    "The model is 51.8% accurate in predicting the dominant topic obtained from Gensim using the DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88cf3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "# Train the classifier on training data set\n",
    "clf = MultinomialNB()\n",
    "%time clf.fit(X_train,y_train) # 6.3s\n",
    "\n",
    "# Prediction accuracy on test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c1205",
   "metadata": {},
   "source": [
    "#### TF IDF Matrix Interpretation:\n",
    "\n",
    "The model is 49.1% accurate in predicting the topic using TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF_IDF MATRIX\n",
    "\n",
    "# Train the classifier on idf data set\n",
    "%time clf.fit(X_train_idf,y_train_idf) # 0.2s\n",
    "y_pref_idf = clf.predict(X_test_idf)\n",
    "\n",
    "# Prediction accuracy\n",
    "clf.score(X_test_idf,y_test_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35adc579",
   "metadata": {},
   "source": [
    "#### Naive Bayes Model Heat Map and Interpretation\n",
    "\n",
    "1. Naive Bayes Model has better Accuracy than TF-IDF Matrix\n",
    "2. Ride is getting confused with Policy, Experience, Availability and Payment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5554c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the confusion matrix and display it as a heatmap.\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "# neat heat map\n",
    "sns.heatmap(conf_mat , cmap=\"RdBu_r\", xticklabels = categories, yticklabels = categories)\n",
    "plt.ylabel('True values')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54bc89",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier Interpretation and Mapping\n",
    "\n",
    "1. The model accuracy is 49.05% for TF and 43.39% for TF_IDF\n",
    "2. The resuls are same as Naive Bayes model heat map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188bc477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "# Train the classifier on training data set\n",
    "clf = clf = RandomForestClassifier(n_estimators=50, max_depth=5,random_state=0)\n",
    "%time clf.fit(X_train,y_train)\n",
    "\n",
    "# Prediction accuracy on test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on TF is : \" + str(clf.score(X_test,y_test)))\n",
    "\n",
    "# Train and test the classifier on idf data set\n",
    "%time clf.fit(X_train_idf,y_train_idf)\n",
    "y_pref_idf = clf.predict(X_test_idf)\n",
    "print(\"Accuracy on TF-IDF is : \" + str(clf.score(X_test_idf,y_test_idf)))\n",
    "\n",
    "# Compute the confusion matrix and display it as a heatmap.\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(conf_mat , cmap=\"Blues\", xticklabels = categories, yticklabels = categories)\n",
    "plt.ylabel('True values')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4ffc8",
   "metadata": {},
   "source": [
    "#### Logistic Regression Interpretation and Mapping\n",
    "\n",
    "1. The model accuracy is 49.05% for TF and 54.71% for TF_IDF. Therefore,this model is giving us better results\n",
    "2. Ride is getting confused with Experience, Advertising & Policy\n",
    "3. In addition to Ride, Advertising & Policy, Experience is also getting confused with App, Pricing and Vehicle\n",
    "\n",
    "##### Recommendations based on Logistic Regression\n",
    "\n",
    "1. To improve experience which is very well correlated with sentiment words like, horrible, last, reliability according to Gensim Model, Uber needs to improve the overall experience of the customer\n",
    "2. Experience is very well confused with Policy, App, Pricing, & Vehicle. So, Uber needs to focus on improving them, to improve the overall experience of the customer\n",
    "3. Ride is also a prominent feature, which according to Gensim Model has keywords like charge, money etc, therefore we need to improve Experience, Adveritsing and Policy\n",
    "4. Policy, according to Gensim model, is related to masks, measure, airport and reports, therefore, Policy-wise, ber needs to improve and advertise it's policies in a better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939608b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier on training data set\n",
    "clf = LogisticRegression(random_state=0)\n",
    "%time clf.fit(X_train,y_train)\n",
    " \n",
    "# Prediction accuracy on test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on TF is : \" + str(clf.score(X_test,y_test)))\n",
    " \n",
    "# Train and test the classifier on idf data set\n",
    "%time clf.fit(X_train_idf,y_train_idf)\n",
    "y_pref_idf = clf.predict(X_test_idf)\n",
    "print(\"Accuracy on TF-IDF is : \" + str(clf.score(X_test_idf,y_test_idf)))\n",
    " \n",
    "# Compute the confusion matrix and display it as a heatmap.\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(conf_mat , cmap=\"RdBu_r\", xticklabels = categories, yticklabels = categories)\n",
    "plt.ylabel('True values')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e54d0c6",
   "metadata": {},
   "source": [
    "### Data Field Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "Days = uber_ride_reviews_df['Day'].value_counts()\n",
    "Days.plot(kind='bar', color = 'blue', figsize=(10,5))\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of Reviews vs Days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32add6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonality\n",
    "Days = uber_ride_reviews_df['Month'].value_counts()\n",
    "Days.plot(kind='bar', color = 'blue', figsize=(10,5))\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of Reviews vs Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9f51a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Year\n",
    "Days = uber_ride_reviews_df['Year'].value_counts()\n",
    "Days.plot(kind='bar', color = 'blue', figsize=(10,5))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of Reviews vs Year')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
